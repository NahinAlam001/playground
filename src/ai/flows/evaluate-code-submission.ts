'use server';

/**
 * @fileOverview Evaluates code submissions using BLEU-4, Custom-BLEU, and ECS metrics to rank user performance.
 *
 * - evaluateCodeSubmission - A function that handles the evaluation process.
 * - EvaluateCodeSubmissionInput - The input type for the evaluateCodeSubmission function.
 * - EvaluateCodeSubmissionOutput - The return type for the evaluateCodeSubmission function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const EvaluateCodeSubmissionInputSchema = z.object({
  referenceSummary: z
    .string()
    .describe('The reference summary to compare the submission output against.'),
  submissionOutput: z
    .string()
    .describe('The output generated by the submitted code.'),
});
export type EvaluateCodeSubmissionInput = z.infer<
  typeof EvaluateCodeSubmissionInputSchema
>;

const EvaluateCodeSubmissionOutputSchema = z.object({
  bleu4Score: z.number().describe('The BLEU-4 score of the submission.'),
  customBleuScore: z
    .number()
    .describe('The Custom-BLEU score of the submission.'),
  entityCoverageScore: z
    .number()
    .describe('The Entity Coverage Score of the submission.'),
  summary: z.string().describe('Summary of the evaluation results.'),
});
export type EvaluateCodeSubmissionOutput = z.infer<
  typeof EvaluateCodeSubmissionOutputSchema
>;

export async function evaluateCodeSubmission(
  input: EvaluateCodeSubmissionInput
): Promise<EvaluateCodeSubmissionOutput> {
  return evaluateCodeSubmissionFlow(input);
}

const evaluateCodeSubmissionPrompt = ai.definePrompt({
  name: 'evaluateCodeSubmissionPrompt',
  input: {schema: EvaluateCodeSubmissionInputSchema},
  output: {schema: EvaluateCodeSubmissionOutputSchema},
  prompt: `You are an AI code evaluation expert. You are given a reference summary and the output of a code submission. Your task is to evaluate the code submission based on the following metrics:\n\n  - BLEU-4: Calculate the BLEU-4 score by comparing the code submission output with the reference summary.\n  - Custom-BLEU: Calculate the Custom-BLEU score by penalizing missing name/job title in the code submission output compared to the reference summary.\n  - Entity Coverage Score (ECS): Calculate the ECS by comparing the entities covered in the code submission output with the reference summary.\n\nReference Summary: {{{referenceSummary}}}\nCode Submission Output: {{{submissionOutput}}}\n\nProvide the BLEU-4 score, Custom-BLEU score, Entity Coverage Score, and a summary of the evaluation results in JSON format.\n\nEnsure that the scores are between 0 and 1.
`,
});

const evaluateCodeSubmissionFlow = ai.defineFlow(
  {
    name: 'evaluateCodeSubmissionFlow',
    inputSchema: EvaluateCodeSubmissionInputSchema,
    outputSchema: EvaluateCodeSubmissionOutputSchema,
  },
  async input => {
    const {output} = await evaluateCodeSubmissionPrompt(input);
    return output!;
  }
);
